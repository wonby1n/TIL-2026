# TIL: LLM Hallucination(환각현상)

## 1. Hallucination(환각현상)이란?

**정의**
LLM(Large Language Model)이 **사실과 다른 정보**, **존재하지 않는 내용**, **근거 없는 추론**을 마치 사실인 것처럼 생성하는 현상

**핵심 원인**

* LLM은 사실 검증이 아니라 **확률적으로 그럴듯한 문장**을 생성함
* 학습 데이터에 없거나 부족한 정보에 대해 추론을 시도함
* 질문이 모호하거나 개방적일수록 발생 가능성 증가

> Hallucination은 LLM의 구조적 한계로, 완전 제거는 어려움

---

## 2. 해결 방안 1: RAG (Retrieval-Augmented Generation)

**개념**
답변 생성 전에 외부 데이터(문서, DB, 벡터스토어)를 검색(Retrieval)하고, 해당 정보를 근거로 답변을 생성(Generation)하는 방식

**효과**

* 모델의 내부 지식이 아닌 **실제 문서 기반 응답**
* 최신 정보 및 도메인 특화 지식 대응 가능
* hallucination을 **가장 효과적으로 감소**

**한계**

* 검색 데이터 품질에 결과가 크게 의존
* 벡터 DB, 임베딩 등 **시스템 설계 비용** 발생

---

## 3. 해결 방안 2: 파인 튜닝(Fine-tuning)

**개념**
특정 도메인 데이터로 모델을 추가 학습시켜 응답 경향을 조정하는 방법

**효과**

* 문체, 용어, 응답 포맷 안정화
* 특정 유형의 오류 감소

**한계**

* 사실 검증 능력을 근본적으로 부여하지는 못함
* 최신 정보나 새로운 지식에는 여전히 취약
* hallucination을 단독으로 해결하는 것은 **불가능**

---

## 4. 해결 방안 3: 프롬프트 엔지니어링

### 4-1. CoT (Chain-of-Thought)

**개념**
문제를 단계적으로 사고하도록 유도하는 프롬프트 기법

**효과**

* 복잡한 문제에서 논리적 일관성 향상
* 추론·계산 오류 감소

**주의 사항**

* hallucination을 완전히 제거하지는 못함
* 잘못된 추론이 길어질 가능성 존재

---

### 4-2. DO NOT 지침 (제약형 프롬프트)

**개념**
모델이 하지 말아야 할 행동을 명확히 제한하는 방식

**예시**

* 모르는 경우 추측하지 말고 "알 수 없습니다"라고 답할 것
* 출처 없는 정보는 생성하지 말 것

**효과**

* 근거 없는 생성 감소
* 응답의 안정성과 보수성 향상

**한계**

* 모델이 지침을 항상 100% 준수하지는 않음

---

## 5. 정리

| 방법         | Hallucination 감소 효과 |
| ---------- | ------------------- |
| RAG        | ★★★★★               |
| 파인 튜닝      | ★★☆☆☆               |
| 프롬프트 엔지니어링 | ★★☆☆☆ (보조 수단)       |

**결론**
Hallucination은 LLM의 구조적 한계이며,
현재 가장 현실적인 대응 방법은 **RAG + 제약형 프롬프트** 조합이다.
